{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 202 - Training and Evaluating CNTK Models in Spark ML Pipelines\n",
        "\n",
        "Yet again, now using the `Word2Vec` Estimator from Spark.  We can use the tree-based\n",
        "learners from spark in this scenario due to the lower dimensionality representation of\n",
        "features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SamplePool",
              "session_id": 44,
              "statement_id": 1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T08:52:34.4472089Z",
              "execution_start_time": "2021-03-15T08:53:03.5150773Z",
              "execution_finish_time": "2021-03-15T08:53:05.5813945Z"
            },
            "text/plain": "StatementMeta(SamplePool, 44, 1, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": ""
          },
          "execution_count": 1,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SamplePool",
              "session_id": 44,
              "statement_id": 2,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T08:52:34.4488616Z",
              "execution_start_time": "2021-03-15T08:53:05.684419Z",
              "execution_finish_time": "2021-03-15T08:53:14.3720511Z"
            },
            "text/plain": "StatementMeta(SamplePool, 44, 2, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "   rating                                               text\n0       5  I LOVED THIS BOOK!  This was my first Jodi Pic...\n1       5  My Sister's Keeper:  A Novel  What a very touc...\n2       4  hooked by chapter one; tear jerker in the end ...\n3       4  A thought-provoking book  A very interesting p...\n4       5  Fiction mimics the future  Very well written, ...\n5       5  Page turner until the end!  This was a fantast...\n6       4  Makes you aware  Makes you aware of some of th...\n7       5  A Hands-on Book for Connecting With Students  ...\n8       5  Dozens of positive, effective strategies  A vi...\n9       5  Hard To Put Down  Harry Bosch and his partner ..."
          },
          "execution_count": 2,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "data = spark.read.parquet(\"wasbs://publicwasb@mmlspark.blob.core.windows.net/BookReviewsFromAmazon10K.parquet\")\n",
        "data.limit(10).toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Modify the label column to predict a rating greater than 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SamplePool",
              "session_id": 44,
              "statement_id": 3,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T08:52:34.4505561Z",
              "execution_start_time": "2021-03-15T08:53:14.4689073Z",
              "execution_finish_time": "2021-03-15T08:53:16.5298028Z"
            },
            "text/plain": "StatementMeta(SamplePool, 44, 3, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                                text  label\n0  I LOVED THIS BOOK!  This was my first Jodi Pic...   True\n1  My Sister's Keeper:  A Novel  What a very touc...   True\n2  hooked by chapter one; tear jerker in the end ...   True\n3  A thought-provoking book  A very interesting p...   True\n4  Fiction mimics the future  Very well written, ...   True"
          },
          "execution_count": 3,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "processedData = data.withColumn(\"label\", data[\"rating\"] > 3) \\\n",
        "                    .select([\"text\", \"label\"])\n",
        "processedData.limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Split the dataset into train, test and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SamplePool",
              "session_id": 44,
              "statement_id": 4,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T08:52:34.4520615Z",
              "execution_start_time": "2021-03-15T08:53:16.6308059Z",
              "execution_finish_time": "2021-03-15T08:53:18.709243Z"
            },
            "text/plain": "StatementMeta(SamplePool, 44, 4, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": ""
          },
          "execution_count": 4,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "train, test, validation = processedData.randomSplit([0.60, 0.20, 0.20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use `Tokenizer` and `Word2Vec` to generate the features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SamplePool",
              "session_id": 44,
              "statement_id": 5,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T08:52:34.4573503Z",
              "execution_start_time": "2021-03-15T08:53:18.8092197Z",
              "execution_finish_time": "2021-03-15T08:53:43.5540332Z"
            },
            "text/plain": "StatementMeta(SamplePool, 44, 5, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": ""
          },
          "execution_count": 5,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import Tokenizer, Word2Vec\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "partitions = train.rdd.getNumPartitions()\n",
        "word2vec = Word2Vec(maxIter=4, seed=42, inputCol=\"words\", outputCol=\"features\",\n",
        "                    numPartitions=partitions)\n",
        "textFeaturizer = Pipeline(stages = [tokenizer, word2vec]).fit(train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Transform each of the train, test and validation datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SamplePool",
              "session_id": 44,
              "statement_id": 6,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T08:52:34.4592118Z",
              "execution_start_time": "2021-03-15T08:53:43.6596751Z",
              "execution_finish_time": "2021-03-15T08:53:47.7885513Z"
            },
            "text/plain": "StatementMeta(SamplePool, 44, 6, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "   label                                           features\n0  False  [0.016259915026960968, 0.028176641930675126, -...\n1  False  [0.029078174612228655, -0.0014457819621408088,...\n2  False  [0.014679526520410071, 0.056803625366754006, 0...\n3  False  [0.053565987438345564, 0.05298001414988763, 0....\n4  False  [0.0192281842113328, 0.02463517978851777, -0.0...\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:2110: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:\n  Unsupported type in conversion to Arrow: VectorUDT\nAttempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true.\n  warnings.warn(msg)"
          },
          "execution_count": 6,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "ptrain = textFeaturizer.transform(train).select([\"label\", \"features\"])\n",
        "ptest = textFeaturizer.transform(test).select([\"label\", \"features\"])\n",
        "pvalidation = textFeaturizer.transform(validation).select([\"label\", \"features\"])\n",
        "ptrain.limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate several models with different parameters from the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SamplePool",
              "session_id": 44,
              "statement_id": 7,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T08:52:34.4610875Z",
              "execution_start_time": "2021-03-15T08:53:47.9280482Z",
              "execution_finish_time": "2021-03-15T08:55:30.3447282Z"
            },
            "text/plain": "StatementMeta(SamplePool, 44, 7, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": ""
          },
          "execution_count": 7,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
        "from mmlspark.train import TrainClassifier\n",
        "import itertools\n",
        "\n",
        "lrHyperParams       = [0.05, 0.2]\n",
        "logisticRegressions = [LogisticRegression(regParam = hyperParam)\n",
        "                       for hyperParam in lrHyperParams]\n",
        "lrmodels            = [TrainClassifier(model=lrm, labelCol=\"label\").fit(ptrain)\n",
        "                       for lrm in logisticRegressions]\n",
        "\n",
        "rfHyperParams       = itertools.product([5, 10], [2, 3])\n",
        "randomForests       = [RandomForestClassifier(numTrees=hyperParam[0], maxDepth=hyperParam[1])\n",
        "                       for hyperParam in rfHyperParams]\n",
        "rfmodels            = [TrainClassifier(model=rfm, labelCol=\"label\").fit(ptrain)\n",
        "                       for rfm in randomForests]\n",
        "\n",
        "gbtHyperParams      = itertools.product([8, 16], [2, 3])\n",
        "gbtclassifiers      = [GBTClassifier(maxBins=hyperParam[0], maxDepth=hyperParam[1])\n",
        "                       for hyperParam in gbtHyperParams]\n",
        "gbtmodels           = [TrainClassifier(model=gbt, labelCol=\"label\").fit(ptrain)\n",
        "                       for gbt in gbtclassifiers]\n",
        "\n",
        "trainedModels       = lrmodels + rfmodels + gbtmodels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Find the best model for the given test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SamplePool",
              "session_id": 44,
              "statement_id": 8,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T08:52:34.4635004Z",
              "execution_start_time": "2021-03-15T08:55:30.4427626Z",
              "execution_finish_time": "2021-03-15T08:55:43.2359391Z"
            },
            "text/plain": "StatementMeta(SamplePool, 44, 8, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "+-------------------+--------------------+\n|false_positive_rate|  true_positive_rate|\n+-------------------+--------------------+\n|                0.0|                 0.0|\n|                0.0|6.211180124223603E-4|\n|                0.0|0.001242236024844...|\n|                0.0|0.001863354037267...|\n|                0.0|0.002484472049689441|\n|                0.0|0.003105590062111801|\n|                0.0|0.003726708074534...|\n|                0.0|0.004347826086956522|\n|                0.0|0.004968944099378882|\n|                0.0|0.005590062111801...|\n|                0.0|0.006211180124223602|\n|                0.0|0.006832298136645...|\n|                0.0|0.007453416149068323|\n|                0.0|0.008074534161490683|\n|                0.0|0.008695652173913044|\n|                0.0|0.009316770186335404|\n|                0.0|0.009937888198757764|\n|                0.0|0.010559006211180125|\n|                0.0|0.011180124223602485|\n|                0.0|0.011801242236024845|\n+-------------------+--------------------+\nonly showing top 20 rows\n\n+---------------+--------------------+------------------+------------------+------------------+------------------+\n|evaluation_type|    confusion_matrix|          accuracy|         precision|            recall|               AUC|\n+---------------+--------------------+------------------+------------------+------------------+------------------+\n| Classification|93.0  295.0   \n41...|0.8318318318318318|0.8417381974248928|0.9745341614906833|0.8526541589293718|\n+---------------+--------------------+------------------+------------------+------------------+------------------+\n\n+--------------------+------------------+--------------------+\n|          model_name|            metric|          parameters|\n+--------------------+------------------+--------------------+\n|TrainClassifier_d...|0.8526541589293718|aggregationDepth:...|\n|TrainClassifier_1...|0.8347121726323877|aggregationDepth:...|\n|TrainClassifier_e...|               0.5|cacheNodeIds: fal...|\n|TrainClassifier_9...|0.7237777742204008|cacheNodeIds: fal...|\n|TrainClassifier_2...|               0.5|cacheNodeIds: fal...|\n|TrainClassifier_f...|0.7197285009925082|cacheNodeIds: fal...|\n|TrainClassifier_0...|0.5593535890375873|cacheNodeIds: fal...|\n|TrainClassifier_f...|0.5848626496766345|cacheNodeIds: fal...|\n|TrainClassifier_e...|0.5717439328936416|cacheNodeIds: fal...|\n|TrainClassifier_d...|0.5813072293014023|cacheNodeIds: fal...|\n+--------------------+------------------+--------------------+"
          },
          "execution_count": 8,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "from mmlspark.automl import FindBestModel\n",
        "bestModel = FindBestModel(evaluationMetric=\"AUC\", models=trainedModels).fit(ptest)\n",
        "bestModel.getEvaluationResults().show()\n",
        "bestModel.getBestModelMetrics().show()\n",
        "bestModel.getAllModelMetrics().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get the accuracy from the validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SamplePool",
              "session_id": 44,
              "statement_id": 9,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T08:52:34.465723Z",
              "execution_start_time": "2021-03-15T08:55:43.3560759Z",
              "execution_finish_time": "2021-03-15T08:55:47.5050002Z"
            },
            "text/plain": "StatementMeta(SamplePool, 44, 9, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "Best model's accuracy on validation set = 82.62%\nBest model's AUC on validation set = 85.43%"
          },
          "execution_count": 9,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "from mmlspark.train import ComputeModelStatistics\n",
        "predictions = bestModel.transform(pvalidation)\n",
        "metrics = ComputeModelStatistics().transform(predictions)\n",
        "print(\"Best model's accuracy on validation set = \"\n",
        "      + \"{0:.2f}%\".format(metrics.first()[\"accuracy\"] * 100))\n",
        "print(\"Best model's AUC on validation set = \"\n",
        "      + \"{0:.2f}%\".format(metrics.first()[\"AUC\"] * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SamplePool",
              "session_id": 44,
              "statement_id": 10,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T08:52:34.4677202Z",
              "execution_start_time": "2021-03-15T08:55:47.5998459Z",
              "execution_finish_time": "2021-03-15T08:56:08.1827333Z"
            },
            "text/plain": "StatementMeta(SamplePool, 44, 10, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": ""
          },
          "execution_count": 10,
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "spark.stop()"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  }
}