{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Classification - Adult Census using Vowpal Wabbit in MMLSpark\n",
        "\n",
        "In this example, we predict incomes from the *Adult Census* dataset using Vowpal Wabbit (VW) classifier in MMLSpark.\n",
        "First, we read the data and split it into train and test sets as in this [example](https://github.com/Azure/mmlspark/blob/master/notebooks/samples/Classification%20-%20Adult%20Census.ipynb\n",
        ")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SamplePool",
              "session_id": 27,
              "statement_id": 1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T05:16:08.4889607Z",
              "execution_start_time": "2021-03-15T05:16:45.4676558Z",
              "execution_finish_time": "2021-03-15T05:17:00.0274338Z"
            },
            "text/plain": "StatementMeta(SamplePool, 27, 1, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "  education       marital-status  hours-per-week  income\n0      10th             Divorced            25.0   <=50K\n1      10th             Divorced            40.0   <=50K\n2      10th             Divorced            40.0   <=50K\n3      10th             Divorced            40.0   <=50K\n4      10th   Married-civ-spouse            16.0   <=50K\n5      10th   Married-civ-spouse            35.0   <=50K\n6      10th   Married-civ-spouse            40.0   <=50K\n7      10th   Married-civ-spouse            40.0   <=50K\n8      10th   Married-civ-spouse            40.0   <=50K\n9      10th   Married-civ-spouse            40.0   <=50K"
          },
          "execution_count": 1,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "data = spark.read.parquet(\"wasbs://publicwasb@mmlspark.blob.core.windows.net/AdultCensusIncome.parquet\")\n",
        "data = data.select([\"education\", \"marital-status\", \"hours-per-week\", \"income\"])\n",
        "train, test = data.randomSplit([0.75, 0.25], seed=123)\n",
        "train.limit(10).toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we define a pipeline that includes feature engineering and training of a VW classifier. We use a featurizer provided by VW that hashes the feature names. \n",
        "Note that VW expects classification labels being -1 or 1. Thus, the income category is mapped to this space before feeding training data into the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SamplePool",
              "session_id": 27,
              "statement_id": 2,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T05:16:08.4904713Z",
              "execution_start_time": "2021-03-15T05:17:00.1365221Z",
              "execution_finish_time": "2021-03-15T05:17:04.266755Z"
            },
            "text/plain": "StatementMeta(SamplePool, 27, 2, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "24412"
          },
          "execution_count": 2,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import when, col\n",
        "from pyspark.ml import Pipeline\n",
        "from mmlspark.vw import VowpalWabbitFeaturizer, VowpalWabbitClassifier\n",
        "\n",
        "# Define classification label\n",
        "train = train.withColumn(\"label\", when(col(\"income\").contains(\"<\"), 0.0).otherwise(1.0)).repartition(1).cache()\n",
        "print(train.count())\n",
        "\n",
        "# Specify featurizer\n",
        "vw_featurizer = VowpalWabbitFeaturizer(inputCols=[\"education\", \"marital-status\", \"hours-per-week\"],\n",
        "                                       outputCol=\"features\")\n",
        "\n",
        "# Define VW classification model\n",
        "args = \"--loss_function=logistic --quiet --holdout_off\"\n",
        "vw_model = VowpalWabbitClassifier(featuresCol=\"features\",\n",
        "                                  labelCol=\"label\",\n",
        "                                  args=args,\n",
        "                                  numPasses=10)\n",
        "\n",
        "# Create a pipeline\n",
        "vw_pipeline = Pipeline(stages=[vw_featurizer, vw_model])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we are ready to train the model by fitting the pipeline with the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SamplePool",
              "session_id": 27,
              "statement_id": 3,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T05:16:08.4919226Z",
              "execution_start_time": "2021-03-15T05:17:04.3626478Z",
              "execution_finish_time": "2021-03-15T05:17:08.494127Z"
            },
            "text/plain": "StatementMeta(SamplePool, 27, 3, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": ""
          },
          "execution_count": 3,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# Train the model\n",
        "vw_trained = vw_pipeline.fit(train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After the model is trained, we apply it to predict the income of each sample in the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SamplePool",
              "session_id": 27,
              "statement_id": 4,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T05:16:08.4940692Z",
              "execution_start_time": "2021-03-15T05:17:08.5900182Z",
              "execution_finish_time": "2021-03-15T05:17:10.6625268Z"
            },
            "text/plain": "StatementMeta(SamplePool, 27, 4, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "  education  ... prediction\n0      10th  ...        0.0\n1      10th  ...        0.0\n2      10th  ...        0.0\n3      10th  ...        0.0\n4      10th  ...        0.0\n5      10th  ...        0.0\n6      10th  ...        0.0\n7      10th  ...        0.0\n8      10th  ...        0.0\n9      10th  ...        0.0\n\n[10 rows x 9 columns]\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:2110: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:\n  Unsupported type in conversion to Arrow: VectorUDT\nAttempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true.\n  warnings.warn(msg)"
          },
          "execution_count": 4,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# Making predictions\n",
        "test = test.withColumn(\"label\", when(col(\"income\").contains(\"<\"), 0.0).otherwise(1.0))\n",
        "prediction = vw_trained.transform(test)\n",
        "prediction.limit(10).toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we evaluate the model performance using `ComputeModelStatistics` function which will compute confusion matrix, accuracy, precision, recall, and AUC by default for classificaiton models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SamplePool",
              "session_id": 27,
              "statement_id": 5,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T05:16:08.4962225Z",
              "execution_start_time": "2021-03-15T05:17:10.7588805Z",
              "execution_finish_time": "2021-03-15T05:17:16.968695Z"
            },
            "text/plain": "StatementMeta(SamplePool, 27, 5, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "  evaluation_type  ...       AUC\n0  Classification  ...  0.698855\n\n[1 rows x 6 columns]\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:2110: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:\n  Unsupported type in conversion to Arrow: MatrixUDT\nAttempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true.\n  warnings.warn(msg)"
          },
          "execution_count": 5,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "from mmlspark.train import ComputeModelStatistics\n",
        "metrics = ComputeModelStatistics(evaluationMetric=\"classification\", \n",
        "                                 labelCol=\"label\", \n",
        "                                 scoredLabelsCol=\"prediction\").transform(prediction)\n",
        "metrics.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SamplePool",
              "session_id": 27,
              "statement_id": 6,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T05:16:08.4982802Z",
              "execution_start_time": "2021-03-15T05:17:17.0662607Z",
              "execution_finish_time": "2021-03-15T05:17:19.1321815Z"
            },
            "text/plain": "StatementMeta(SamplePool, 27, 6, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": ""
          },
          "execution_count": 6,
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "spark.stop()"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  }
}