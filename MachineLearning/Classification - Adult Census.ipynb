{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 101 - Training and Evaluating Classifiers with `mmlspark`\n",
        "\n",
        "In this example, we try to predict incomes from the *Adult Census* dataset.\n",
        "\n",
        "First, we import the packages (use `help(mmlspark)` to view contents),"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SamplePool",
              "session_id": 28,
              "statement_id": 1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T05:19:31.3567503Z",
              "execution_start_time": "2021-03-15T05:20:12.4894902Z",
              "execution_finish_time": "2021-03-15T05:20:14.5498341Z"
            },
            "text/plain": "StatementMeta(SamplePool, 28, 1, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": ""
          },
          "execution_count": 1,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's read the data and split it to train and test sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SamplePool",
              "session_id": 28,
              "statement_id": 2,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T05:19:31.3581093Z",
              "execution_start_time": "2021-03-15T05:20:14.650666Z",
              "execution_finish_time": "2021-03-15T05:20:25.1264062Z"
            },
            "text/plain": "StatementMeta(SamplePool, 28, 2, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "  education       marital-status  hours-per-week  income\n0      10th             Divorced             1.0   <=50K\n1      10th             Divorced            40.0   <=50K\n2      10th             Divorced            40.0   <=50K\n3      10th             Divorced            40.0   <=50K\n4      10th             Divorced            40.0   <=50K\n5      10th             Divorced            40.0   <=50K\n6      10th   Married-civ-spouse            20.0   <=50K\n7      10th   Married-civ-spouse            40.0   <=50K\n8      10th   Married-civ-spouse            40.0   <=50K\n9      10th   Married-civ-spouse            40.0   <=50K"
          },
          "execution_count": 2,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "data = spark.read.parquet(\"wasbs://publicwasb@mmlspark.blob.core.windows.net/AdultCensusIncome.parquet\")\n",
        "data = data.select([\"education\", \"marital-status\", \"hours-per-week\", \"income\"])\n",
        "train, test = data.randomSplit([0.75, 0.25], seed=123)\n",
        "train.limit(10).toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`TrainClassifier` can be used to initialize and fit a model, it wraps SparkML classifiers.\n",
        "You can use `help(mmlspark.TrainClassifier)` to view the different parameters.\n",
        "\n",
        "Note that it implicitly converts the data into the format expected by the algorithm: tokenize\n",
        "and hash strings, one-hot encodes categorical variables, assembles the features into a vector\n",
        "and so on.  The parameter `numFeatures` controls the number of hashed features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SamplePool",
              "session_id": 28,
              "statement_id": 3,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T05:19:31.3593322Z",
              "execution_start_time": "2021-03-15T05:20:25.2345046Z",
              "execution_finish_time": "2021-03-15T05:20:44.0916129Z"
            },
            "text/plain": "StatementMeta(SamplePool, 28, 3, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": ""
          },
          "execution_count": 3,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "from mmlspark.train import TrainClassifier\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "model = TrainClassifier(model=LogisticRegression(), labelCol=\"income\", numFeatures=256).fit(train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After the model is trained, we score it against the test dataset and view metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SamplePool",
              "session_id": 28,
              "statement_id": 4,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T05:19:31.3607251Z",
              "execution_start_time": "2021-03-15T05:20:44.1899637Z",
              "execution_finish_time": "2021-03-15T05:20:48.3528184Z"
            },
            "text/plain": "StatementMeta(SamplePool, 28, 4, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "  evaluation_type  ...       AUC\n0  Classification  ...  0.865245\n\n[1 rows x 6 columns]\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:2110: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:\n  Unsupported type in conversion to Arrow: MatrixUDT\nAttempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true.\n  warnings.warn(msg)"
          },
          "execution_count": 4,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "from mmlspark.train import ComputeModelStatistics, TrainedClassifierModel\n",
        "prediction = model.transform(test)\n",
        "metrics = ComputeModelStatistics().transform(prediction)\n",
        "metrics.limit(10).toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we save the model so it can be used in a scoring program."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SamplePool",
              "session_id": 28,
              "statement_id": 5,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T05:19:31.3621401Z",
              "execution_start_time": "2021-03-15T05:20:48.4666094Z",
              "execution_finish_time": "2021-03-15T05:21:15.3846889Z"
            },
            "text/plain": "StatementMeta(SamplePool, 28, 5, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": ""
          },
          "execution_count": 5,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "model.write().overwrite().save(\"AdultCensus.mml\") # save to abfss with simple data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SamplePool",
              "session_id": 28,
              "statement_id": 7,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T05:21:48.1987124Z",
              "execution_start_time": "2021-03-15T05:21:48.2995778Z",
              "execution_finish_time": "2021-03-15T05:21:50.3708005Z"
            },
            "text/plain": "StatementMeta(SamplePool, 28, 7, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": ""
          },
          "execution_count": 7,
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "spark.stop()"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  }
}